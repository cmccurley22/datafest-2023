---
title: "kate-exploring"
output: github_document
date: "2023-04-14"
---

```{r setup, include=FALSE}
library(tidyverse)
# knitr options
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

```{r read-data}
df_attorneys <- read_csv("./data/attorneys.csv")
df_attorneytimeentries <- read_csv("./data/attorneytimeentries.csv")
df_categories <- read_csv("./data/categories.csv")
df_clients <- read_csv("./data/clients.csv")
df_questionposts <- read_csv("./data/questionposts.csv")
df_questions <- read_csv("./data/questions.csv")
df_statesites <- read_csv("./data/statesites.csv")
df_subcategories <- read_csv("./data/subcategories.csv")

```

```{r}
df_zip <- read_csv("./uszips.csv") %>%
  rename(PostalCode = zip)
df_zip
```

## Population Data

```{r}
df_pop_in <- read_csv("./data/ACSDT5Y2019.B01003-Data.csv", skip = 1)


df_pop_narrow <- select(df_pop_in, c("Geography", "Geographic Area Name", "Estimate!!Total"))

df_pop_named <-
  df_pop_narrow %>% 
  rename(id = Geography) %>% 
  separate(
    col = `Geographic Area Name`,
    into = c("County", "state"),
    sep = (",") 
  ) %>%
  mutate_if(is.character, str_trim)

# df_pop_named

st_crosswalk <- tibble(state = state.name) %>%
   bind_cols(tibble(abb = state.abb)) %>% 
   bind_rows(tibble(state = "District of Columbia", abb = "DC"))
# st_crosswalk
 
df_population <- right_join(df_pop_named, st_crosswalk, by = "state")
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
df_attorneytimeentries %>%
  ggplot(aes(y = StateAbbr, x = Hours)) +
  geom_point()

df_attorneytimeentries %>% count(StateAbbr)
```

```{r}
df_attorney_data <-
  merge(df_attorneys, df_attorneytimeentries, by = c("AttorneyUno", "StateAbbr")) %>%
  subset(select = -c(Id.x, Id.y, CreatedUtc, EnteredOnUtc, TimeEntryUno))


df_attorney_data
```

```{r}
df_question_data <-
  merge(df_questions, df_questionposts, by = c("QuestionUno", "StateAbbr")) %>%
  subset(select = -c(Id.x, Id.y, CategoryUno, SubcategoryUno, TakenOnUtc, ClosedOnUtc, LegalDeadline, CreatedUtc))

df_question_data <-
  df_question_data %>%
  rename(ClientUno = AskedByClientUno)

df_question_data <-
  df_question_data %>%
  rename(AttorneyUno = ClosedByAttorneyUno)

df_question_data
```

```{r}
df_categories_data <-
  merge(df_categories, df_subcategories, by = c("CategoryUno", "StateAbbr")) %>%
  subset(select = -c(Id.x, Id.y))


df_categories_data
```

```{r}
df_client_state_data <-
  merge(df_clients, df_statesites, by = c("StateAbbr", "StateName")) %>%
  subset(select = -c(Id.x, Id.y, CreatedUtc))

df_client_state_data
```

```{r}

# 
# df_client_state_data <-
#   df_client_state_data %>%
#   rename(ClientCreatedUtc = CreatedUtc)
```

```{r}
big_data_1 <-
  merge(df_question_data, df_client_state_data, by = c("ClientUno", "StateAbbr")) %>%
  subset(select = -c(QuestionUno, StateName))

big_data_1
```

```{r}
df_zip_tz <-
  left_join(big_data_1, df_zip, by=("PostalCode"))
df_zip_tz

df_sum_tz <-
  df_zip_tz %>% 
  summarise(PostalCode, timezone)
df_sum_tz

df_sum_tz_unique = df_sum_tz[!duplicated(df_sum_tz), ]
df_sum_tz_unique
```

```{r}
library(lubridate)
library(timechange)
```

```{r}
chicago <-
  df_zip_tz %>% 
  filter(timezone == "America/Chicago")
date = force_tzs(chicago$AskedOnUtc, tzones = "UTC")
chicago["Actual Time"] = strftime(date, format = "%Y/%m/%d %H:%M:%S", tz = "America/Chicago")
chicago

indiana <-
  df_zip_tz %>% 
  filter(timezone == "America/Indiana/Indianapolis")
date = force_tzs(indiana$AskedOnUtc, tzones = "UTC")
indiana["Actual Time"] = strftime(date, format = "%Y/%m/%d %H:%M:%S", tz = "America/Indiana/Indianapolis")
indiana

la <-
  df_zip_tz %>% 
  filter(timezone == "America/Los_Angeles")
date = force_tzs(la$AskedOnUtc, tzones = "UTC")
la["Actual Time"] = strftime(date, format = "%Y/%m/%d %H:%M:%S", tz = "America/Los_Angeles")
la

detroit <-
  df_zip_tz %>% 
  filter(timezone == "America/Detroit")
date = force_tzs(detroit$AskedOnUtc, tzones = "UTC")
detroit["Actual Time"] = strftime(date, format = "%Y/%m/%d %H:%M:%S", tz = "America/Detroit")
detroit

ny <-
  df_zip_tz %>% 
  filter(timezone == "America/New_York")
date = force_tzs(ny$AskedOnUtc, tzones = "UTC")
ny["Actual Time"] = strftime(date, format = "%Y/%m/%d %H:%M:%S", tz = "America/New_York")
ny

denver <-
  df_zip_tz %>% 
  filter(timezone == "America/New_York")
date = force_tzs(denver$AskedOnUtc, tzones = "UTC")
denver["Actual Time"] = strftime(date, format = "%Y/%m/%d %H:%M:%S", tz = "America/Denver")
denver

phoenix <-
  df_zip_tz %>% 
  filter(timezone == "America/Phoenix")
date = force_tzs(phoenix$AskedOnUtc, tzones = "UTC")
phoenix["Actual Time"] = strftime(date, format = "%Y/%m/%d %H:%M:%S", tz = "America/Phoenix")
phoenix

toronto <-
  df_zip_tz %>% 
  filter(timezone == "America/Toronto")
date = force_tzs(toronto$AskedOnUtc, tzones = "UTC")
toronto["Actual Time"] = strftime(date, format = "%Y/%m/%d %H:%M:%S", tz = "America/Toronto")
toronto

honolulu <-
  df_zip_tz %>% 
  filter(timezone == "Pacific/Honolulu")
date = force_tzs(honolulu$AskedOnUtc, tzones = "UTC")
honolulu["Actual Time"] = strftime(date, format = "%Y/%m/%d %H:%M:%S", tz = "Pacific/Honolulu")
honolulu
```

## Time Zone Merge

```{r}
all_time <-
  full_join(chicago, denver) %>% 

  full_join(detroit) %>% 
  full_join(ny) %>% 
  full_join(la) %>% 
  full_join(toronto) %>% 
  full_join(honolulu) %>%
  full_join(phoenix) %>% 
  full_join(indiana)
all_time
```

```{r}
library(tidyr)
timesplit <-
  indiana %>%
  separate("Actual Time", into = c("year", "month", "day", "hour", "minutes", "seconds")) %>% 
  ggplot(mapping = aes(hour)) +
  geom_bar()
timesplit
```

## Divorce Count per County

```{r}
#str_detect(PostText, "[Ee]x[ -]")
all_timesplit <-
  all_time %>%
  mutate(cat = str_detect(Subcategory, "[Dd]ivorce")) %>% 
  filter(cat == "TRUE") %>% 
  separate("Actual Time", into = c("year", "month", "day", "hour", "minutes", "seconds")) 
 
all_timesplit
```

```{r}

div_county <- 
  all_timesplit %>% 
    group_by(county_fips, StateAbbr) %>% 
    count(Subcategory) 

div_county

div_dem <- 
   all_timesplit %>% 
    # group_by(county_fips, StateAbbr) %>% 
    select(c(county_fips, StateAbbr, EthnicIdentity, Age, Gender, MaritalStatus)) %>% 
  group_by(StateAbbr) %>% 
  count(EthnicIdentity) %>% 
  arrange(desc(n))

div_dem_fl <- 
  div_dem %>% 
    mutate(other = str_detect(EthnicIdentity, "[Oo]ther"),
           multi = str_detect(EthnicIdentity, ",")) %>% 
    filter(StateAbbr == "FL")

div_ag <- 
  all_timesplit %>% 
  select(c(county_fips, StateAbbr, Age, Gender, MaritalStatus)) %>%
    group_by(StateAbbr) %>% 
    count(Gender) %>% 
    arrange(desc(n))

div_ag
  

```

```{r}
all_timesplit %>% 
  group_by(hour, StateAbbr) %>% 
  count(hour) %>% 
  ggplot(mapping = aes(hour, n, color = StateAbbr)) +
  geom_point()
```

## Age vs Number of Posts about Divorce etc.

```{r}
div_ag %>% 
  ggplot(mapping = aes(n, StateAbbr, color = Gender)) +
    geom_point()
```

```{r}
# ALL_DATA <-
#   merge(big_data_1, df_attorney_data, by = c("AttorneyUno", "StateAbbr"))
# 
# ALL_DATA
```

```{r}
write.csv(big_data_1, "C:\\Users\\ldao\\Downloads\\text.csv", row.names=FALSE)
```

```{r}
text_data <-
  big_data_1 %>% 
  mutate(cat = str_detect(PostText, "[Dd]ivorce|[Aa]limony|[Ss]ep[ea]ratio|[Ss]ep[ea]rated| [Ee]x |[Pp]renup|[Cc]hild [Ss]upport|[Cc]ustody")) %>% 
  filter(cat == "TRUE") %>% 
  summarise(PostText, ClientUno)
text_data
write.csv(text_data, "C:\\Users\\ldao\\Downloads\\keyword_aba.csv")
```

```{r}
post_text <-
  text_data %>% 
  summarise(PostText)

post_text_trim <-
  post_text %>% 
  head(200)
post_text_trim

library("stopwords")
library("wordcloud")
library(readr)
library(e1071)
library(mlbench)
library(tm)
library(SnowballC)
library("wordcloud")
library("RColorBrewer")

#library("tidytext")


corpus = Corpus(VectorSource(post_text_trim$PostText))
#Conversion to Lowercase
corpus = tm_map(corpus, PlainTextDocument)
corpus = tm_map(corpus, tolower)
 
#Removing Punctuation
corpus = tm_map(corpus, removePunctuation)
 
#Remove stopwords
corpus = tm_map(corpus, removeWords, c("cloth", stopwords("english")))
 
# Eliminate white spaces
corpus = tm_map(corpus, stripWhitespace)

DTM <- TermDocumentMatrix(corpus)
mat <- as.matrix(DTM)
f <- sort(rowSums(mat),decreasing=TRUE)
dat <- data.frame(word = names(f),freq=f)
head(dat, 5)
```

```{r}
set.seed(100)
wordcloud(words = dat$word, freq = dat$freq, , min.freq = 15, max.words=120, random.order=TRUE, scale = c(4, .25), colors=brewer.pal(8, "Set2"))
```

```{r}
reddit_text <-
  read_csv("./reddit_data/bigcsv.csv")

reddit = Corpus(VectorSource(reddit_text))
#Conversion to Lowercase
reddit = tm_map(reddit, PlainTextDocument)
reddit = tm_map(reddit, tolower)
 
#Removing Punctuation
reddit = tm_map(reddit, removePunctuation)
 
#Remove stopwords
reddit = tm_map(reddit, removeWords, c("cloth", stopwords("english")))
 
# Eliminate white spaces
reddit = tm_map(reddit, stripWhitespace)

DTM_r <- TermDocumentMatrix(reddit)
mat_r <- as.matrix(DTM_r)
f_r <- sort(rowSums(mat_r),decreasing=TRUE)
dat_r <- data.frame(word = names(f_r),freq=f_r)
head(dat_r, 5)
```

```{r}
set.seed(100)
wordcloud(words = dat_r$word, freq = dat_r$freq, , min.freq = 15, max.words=120, random.order=TRUE, scale = c(4, .15), colors=brewer.pal(8, "Set2"))
```
